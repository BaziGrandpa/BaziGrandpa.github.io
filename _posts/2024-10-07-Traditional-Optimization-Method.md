---
layout: page
title:  "Traditional optimization method"
subtitle: "Unconstrained optimization"
date:   2024-10-07 21:21:21 +0530
categories: ["Stochastic Optimization"]
---

One usually distinguishes between two types of optimization problems: numerical and analytical. However here we disdinguish between **unconstrained** and **constrained** optimization problems. In this post we will focus on unconstrained optimization problems. 
## Gradient descent

Making Taylor expansion of the function $f(x)$ around the point $x_0$ one get

<p style="text-align: center;">
$f(x) \approx f(x_0) + \nabla f(x_0)^T(x-x_0)$
</p>

`a super long quote that will break the line and make it look ugly`

## Newton's method
